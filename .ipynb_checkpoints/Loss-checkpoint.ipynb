{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss in Caffe is computed by the Forward pass of the network. \n",
    "\n",
    "caffe中损失是在前向传播过程中得到的。\n",
    "\n",
    "Each layer takes a set of input (bottom) blobs and produces a set of output (top) blobs. \n",
    "每一层获取一系列输入blobs(bottom)，产生一系列输出blobs(top)。\n",
    "\n",
    "Some of these layers’ outputs may be used in the loss function. A typical choice of loss function for one-versus-all classification tasks is the SoftmaxWithLoss function, used in a network definition as follows, for example:\n",
    "\n",
    "一些曾的输出将会被用到损失函数中去。一对多分类中常用的损失函数是SoftmaxWithLoss函数，比如：\n",
    "\n",
    "layer {\n",
    "\n",
    "  name: \"loss\"\n",
    "  \n",
    "  type: \"SoftmaxWithLoss\"\n",
    "  \n",
    "  bottom: \"pred\"\n",
    "  \n",
    "  bottom: \"label\"\n",
    "  \n",
    "  top: \"loss\"\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss weights\n",
    "\n",
    "For nets with multiple layers producing a loss (e.g., a network that both classifies the input using a SoftmaxWithLoss layer and reconstructs it using a EuclideanLoss layer), loss weights can be used to specify their relative importance.\n",
    "\n",
    "对于那些有多层输出损失的网络（比如，一个网络既使用SoftmaxWithLoss层来对输入风雷，又用EuclideanLoss层进行回归），损失权重主要用来标兵它们的相对重要性。\n",
    "\n",
    "By convention, Caffe layer types with the suffix Loss contribute to the loss function, but other layers are assumed to be purely used for intermediate computations.\n",
    "一般情况下，如果caffe中层的类型中以Loss为前缀，则表明该层的输出对于损失函数有贡献，而其他层则认为它们对损失函数没有任何贡献，只做计算。\n",
    "\n",
    "However, any layer can be used as a loss by adding a field loss_weight: <float> to a layer definition for each top blob produced by the layer. \n",
    "然而，只要通过在层中添加一个叫做loss_weight(<float>)的field，任何层都可以为损失做贡献。\n",
    "\n",
    "Layers with the suffix Loss have an implicit loss_weight: 1 for the first top blob (and loss_weight: 0 for any additional tops); other layers have an implicit loss_weight: 0 for all tops. So, the above SoftmaxWithLoss layer could be equivalently written as:\n",
    "\n",
    "层的类型中以Loss为前缀，意味对于该层的第一个top blob，它的loss_weight为1，其他的tops的loss_weight为0；而其他层的所有的tops的loss_weight都为0。因此上述SoftmaxWithLoss层还可以有如下写法：\n",
    "\n",
    "layer {\n",
    "\n",
    "  name: \"loss\"\n",
    "  \n",
    "  type: \"SoftmaxWithLoss\"\n",
    "  \n",
    "  bottom: \"pred\"\n",
    "  \n",
    "  bottom: \"label\"\n",
    "  \n",
    "  top: \"loss\"\n",
    "  \n",
    "  loss_weight: 1\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final loss in Caffe, then, is computed by summing the total weighted loss over the network, as in the following pseudo-code:\n",
    "\n",
    "Caffe中的最终损失，是对整个网络的总的加权损失，用伪代码表示如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8a983cf9283a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "for layer in layers:\n",
    "  for top, loss_weight in layer.tops, layer.loss_weights:\n",
    "    loss += loss_weight * sum(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
